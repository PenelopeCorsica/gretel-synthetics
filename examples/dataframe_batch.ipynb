{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame Batch Training\n",
    "\n",
    "This notebook explores the new batch training feature in Gretel Synthetics. This interface will create N synthetic training configurations, where N is a specific number of batches of column names. We break down the source DataFrame into smaller DataFrames that have the same number of rows, but only a subset of total columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using Colab, you may wish to mount your Google Drive, once that is done, you can create a symlinked\n",
    "# directory that you can use to store the checkpoint directories in.\n",
    "#\n",
    "# For this example we are using some Google data that can be learned and trained relatively quickly\n",
    "# \n",
    "# NOTE: Gretel Synthetic paths must NOT contain whitespaces, which is why we have to symlink to a more local directory\n",
    "# in /content. Unfortunately, Google Drive mounts contain whitespaces either in the \"My drive\" or \"Shared drives\" portion\n",
    "# of the path\n",
    "#\n",
    "# !ln -s \"/content/drive/Shared drives[My Drive]/YOUR_TARGET_DIRECTORY\" checkpoints\n",
    "#\n",
    "# !pip install -U gretel-synthetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gretel_synthetics.batch import DataFrameBatch\n",
    "\n",
    "source_df = pd.read_csv(\"https://gretel-public-website.s3-us-west-2.amazonaws.com/datasets/notebooks/google_marketplace_analytics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(20000, 34)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "source_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create a dict with our config params, these are identical to when creating a LocalConfig object\n",
    "#\n",
    "# NOTE: We do not specify a ``input_data_path`` as this is automatically created for each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "checkpoint_dir = str(Path.cwd() / \"checkpoints\")\n",
    "\n",
    "config_template = {\n",
    "    \"max_line_len\": 2048,\n",
    "    \"vocab_size\": 200000,\n",
    "    \"field_delimiter\": \",\",\n",
    "    \"overwrite\": True,\n",
    "    \"checkpoint_dir\": checkpoint_dir\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-01-28 19:54:13,106 : MainThread : INFO : Creating directory structure for batch jobs...\n"
     ]
    }
   ],
   "source": [
    "# Create our batch handler. During construction, checkpoint directories are automatically created\n",
    "# based on the configured batch size\n",
    "batcher = DataFrameBatch(df=source_df, config=config_template)\n",
    "\n",
    "# Optionally, you can also provide your own batches, which can be a list of lists of strings:\n",
    "#\n",
    "# my_batches = [[\"col1\", \"col2\"], [\"col3\", \"col4\", \"col5\"]]\n",
    "# batcher = DataFrameBatch(df=source_df, batch_headers=my_batches, config=config_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-01-28 19:54:17,835 : MainThread : INFO : Generating training DF and CSV for batch 0\n",
      "2021-01-28 19:54:18,070 : MainThread : INFO : Generating training DF and CSV for batch 1\n",
      "2021-01-28 19:54:18,142 : MainThread : INFO : Generating training DF and CSV for batch 2\n"
     ]
    }
   ],
   "source": [
    "# Next we generate our actual training DataFrames and Training text files\n",
    "#\n",
    "# Each batch directory will now have it's own \"train.csv\" file\n",
    "# Each Batch object now has a ``training_df`` associated with it\n",
    "batcher.create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "racy: 0.9035\n",
      "Epoch 18/100\n",
      "151/151 [==============================] - 6s 34ms/step - loss: 0.3041 - accuracy: 0.9037\n",
      "Epoch 19/100\n",
      "151/151 [==============================] - 6s 34ms/step - loss: 0.3028 - accuracy: 0.9036\n",
      "Epoch 20/100\n",
      "151/151 [==============================] - 6s 34ms/step - loss: 0.3040 - accuracy: 0.9040\n",
      "Epoch 21/100\n",
      "151/151 [==============================] - 6s 34ms/step - loss: 0.3028 - accuracy: 0.9037\n",
      "Epoch 22/100\n",
      "151/151 [==============================] - 6s 34ms/step - loss: 0.3019 - accuracy: 0.9042\n",
      "Epoch 23/100\n",
      "151/151 [==============================] - 6s 34ms/step - loss: 0.3027 - accuracy: 0.9045\n",
      "Epoch 24/100\n",
      "151/151 [==============================] - 6s 34ms/step - loss: 0.3032 - accuracy: 0.9041\n",
      "Epoch 25/100\n",
      "151/151 [==============================] - 6s 35ms/step - loss: 0.3030 - accuracy: 0.9042\n",
      "Epoch 26/100\n",
      "151/151 [==============================] - 6s 35ms/step - loss: 0.3017 - accuracy: 0.9044\n",
      "2021-01-28 19:57:36,335 : MainThread : INFO : Saving model history to model_history.csv\n",
      "2021-01-28 19:57:36,445 : MainThread : INFO : Saving model to /home/temesghen/projects/gretel-synthetics/examples/checkpoints/batch_0/synthetic\n",
      "2021-01-28 19:57:36,446 : MainThread : INFO : Loading training data from /home/temesghen/projects/gretel-synthetics/examples/checkpoints/batch_1/train.csv\n",
      "2021-01-28 19:57:36,511 : MainThread : INFO : Training SentencePiece tokenizer\n",
      "2021-01-28 19:57:36,763 : MainThread : INFO : Loading tokenizer from: m.model\n",
      "2021-01-28 19:57:36,767 : MainThread : INFO : Tokenizer model vocabulary size: 671 tokens\n",
      "2021-01-28 19:57:36,767 : MainThread : INFO : Mapping first line of training data\n",
      "\n",
      "'1<d>1.0<d>1.0<d><d><d><d><d><d>1<d><d><n>'\n",
      " ---- sample tokens mapped to pieces ---- > \n",
      "▁1, <d>, 1, ., 0, <d>, 1, ., 0, <d>, <d>, <d>, <d>, <d>, <d>, 1, <d>, <d>, <n>\n",
      "\n",
      "2021-01-28 19:57:36,770 : MainThread : INFO : Mapping first line of training data\n",
      "\n",
      "'1<d>1.0<d>1.0<d><d><d><d><d><d>1<d><d><n>'\n",
      " ---- sample tokens mapped to int ---- > \n",
      "8, 4, 5, 7, 6, 4, 5, 7, 6, 4, 4, 4, 4, 4, 4, 5, 4, 4, 3\n",
      "\n",
      "2021-01-28 19:57:36,778 : MainThread : INFO : Tokenizing training data\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 77987.35it/s]\n",
      "2021-01-28 19:57:37,037 : MainThread : INFO : Creating and shuffling tensorflow dataset\n",
      "2021-01-28 19:57:37,975 : MainThread : INFO : Initializing synthetic model\n",
      "2021-01-28 19:57:38,390 : MainThread : INFO : Using keras.optimizers.RMSprop optimizer\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (64, None, 256)           171776    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (64, None, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (64, None, 256)           525312    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (64, None, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (64, None, 256)           525312    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (64, None, 256)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (64, None, 671)           172447    \n",
      "=================================================================\n",
      "Total params: 1,394,847\n",
      "Trainable params: 1,394,847\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "66/66 [==============================] - 5s 30ms/step - loss: 2.3798 - accuracy: 0.4261\n",
      "Epoch 2/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.8620 - accuracy: 0.7630\n",
      "Epoch 3/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.6532 - accuracy: 0.8251\n",
      "Epoch 4/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.4813 - accuracy: 0.8803\n",
      "Epoch 5/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.4352 - accuracy: 0.8920\n",
      "Epoch 6/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3925 - accuracy: 0.8999\n",
      "Epoch 7/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3882 - accuracy: 0.9002\n",
      "Epoch 8/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3677 - accuracy: 0.9048\n",
      "Epoch 9/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3523 - accuracy: 0.9085\n",
      "Epoch 10/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3464 - accuracy: 0.9107\n",
      "Epoch 11/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3441 - accuracy: 0.9111\n",
      "Epoch 12/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3383 - accuracy: 0.9123\n",
      "Epoch 13/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3262 - accuracy: 0.9162\n",
      "Epoch 14/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3356 - accuracy: 0.9131\n",
      "Epoch 15/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3255 - accuracy: 0.9158\n",
      "Epoch 16/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3206 - accuracy: 0.9181\n",
      "Epoch 17/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3137 - accuracy: 0.9200\n",
      "Epoch 18/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3107 - accuracy: 0.9198\n",
      "Epoch 19/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3196 - accuracy: 0.9185\n",
      "Epoch 20/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3140 - accuracy: 0.9206\n",
      "Epoch 21/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3070 - accuracy: 0.9223\n",
      "Epoch 22/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3077 - accuracy: 0.9221\n",
      "Epoch 23/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3070 - accuracy: 0.9222\n",
      "Epoch 24/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3014 - accuracy: 0.9227\n",
      "Epoch 25/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3043 - accuracy: 0.9224\n",
      "Epoch 26/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3034 - accuracy: 0.9230\n",
      "Epoch 27/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3011 - accuracy: 0.9232\n",
      "Epoch 28/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3029 - accuracy: 0.9230\n",
      "Epoch 29/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3060 - accuracy: 0.9223\n",
      "Epoch 30/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3006 - accuracy: 0.9240\n",
      "Epoch 31/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3031 - accuracy: 0.9228\n",
      "Epoch 32/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.2989 - accuracy: 0.9237\n",
      "Epoch 33/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.2997 - accuracy: 0.9239\n",
      "Epoch 34/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3028 - accuracy: 0.9233\n",
      "Epoch 35/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3086 - accuracy: 0.9218\n",
      "Epoch 36/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3026 - accuracy: 0.9236\n",
      "Epoch 37/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3053 - accuracy: 0.9226\n",
      "Epoch 38/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3003 - accuracy: 0.9239\n",
      "Epoch 39/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.2982 - accuracy: 0.9243\n",
      "Epoch 40/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3010 - accuracy: 0.9237\n",
      "Epoch 41/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3044 - accuracy: 0.9228\n",
      "Epoch 42/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3000 - accuracy: 0.9241\n",
      "Epoch 43/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.2991 - accuracy: 0.9246\n",
      "Epoch 44/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.2984 - accuracy: 0.9248\n",
      "Epoch 45/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3017 - accuracy: 0.9237\n",
      "Epoch 46/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3020 - accuracy: 0.9239\n",
      "Epoch 47/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.2981 - accuracy: 0.9247\n",
      "Epoch 48/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3006 - accuracy: 0.9244\n",
      "Epoch 49/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3050 - accuracy: 0.9237\n",
      "Epoch 50/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.2946 - accuracy: 0.9257\n",
      "Epoch 51/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3004 - accuracy: 0.9242\n",
      "Epoch 52/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.2991 - accuracy: 0.9244\n",
      "Epoch 53/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3018 - accuracy: 0.9243\n",
      "Epoch 54/100\n",
      "66/66 [==============================] - 2s 28ms/step - loss: 0.2997 - accuracy: 0.9248\n",
      "Epoch 55/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.2912 - accuracy: 0.9267\n",
      "Epoch 56/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.2971 - accuracy: 0.9254\n",
      "Epoch 57/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.2946 - accuracy: 0.9259\n",
      "Epoch 58/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.2992 - accuracy: 0.9244\n",
      "Epoch 59/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.2969 - accuracy: 0.9252\n",
      "Epoch 60/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.2979 - accuracy: 0.9245\n",
      "Epoch 61/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.2989 - accuracy: 0.9254\n",
      "Epoch 62/100\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.3016 - accuracy: 0.9243\n",
      "2021-01-28 20:00:11,431 : MainThread : INFO : Saving model history to model_history.csv\n",
      "2021-01-28 20:00:11,434 : MainThread : INFO : Saving model to /home/temesghen/projects/gretel-synthetics/examples/checkpoints/batch_1/synthetic\n",
      "2021-01-28 20:00:11,435 : MainThread : INFO : Loading training data from /home/temesghen/projects/gretel-synthetics/examples/checkpoints/batch_2/train.csv\n",
      "2021-01-28 20:00:11,488 : MainThread : INFO : Training SentencePiece tokenizer\n",
      "2021-01-28 20:00:12,426 : MainThread : INFO : Loading tokenizer from: m.model\n",
      "2021-01-28 20:00:12,439 : MainThread : INFO : Tokenizer model vocabulary size: 10800 tokens\n",
      "2021-01-28 20:00:12,441 : MainThread : INFO : Mapping first line of training data\n",
      "\n",
      "'<d><d><d>(not set)<d><d><d>referral<d>/od/Things-To-Do-in-Silicon-Valley/fl/How-To-Visit-the-Googleplex-the-Google-Head-Office-in-Mountain-View.htm<d>siliconvalley.about.com<d>1<d>1483857382<n>'\n",
      " ---- sample tokens mapped to pieces ---- > \n",
      "▁, <d>, <d>, <d>, (, n, ot, ▁, se, t, ), <d>, <d>, <d>, r, e, fer, r, al, <d>, /, o, d, /, T, hings, -, T, o, -, D, o, -, i, n, -, Si, li, co, n, -, V, al, le, y, /, f, l, /, H, o, w, -, T, o, -, V, is, i, t, -, t, h, e, -, Google, pl, e, x, -, the, -, Google, -, H, ead, -, O, f, fi, c, e, -, in, -, M, o, u, n, ta, in, -, View, ., htm, <d>, s, ili, con, v, al, ley, ., about, ., co, m, <d>, 1, <d>, 1, 483, 857, 382, <n>\n",
      "\n",
      "2021-01-28 20:00:12,441 : MainThread : INFO : Mapping first line of training data\n",
      "\n",
      "'<d><d><d>(not set)<d><d><d>referral<d>/od/Things-To-Do-in-Silicon-Valley/fl/How-To-Visit-the-Googleplex-the-Google-Head-Office-in-Mountain-View.htm<d>siliconvalley.about.com<d>1<d>1483857382<n>'\n",
      " ---- sample tokens mapped to int ---- > \n",
      "8, 4, 4, 4, 6, 9, 10, 11, 7, 4, 4, 4, 32, 31, 28, 29, 4, 13, 5, 67, 13, 39, 301, 38, 39, 5, 38, 53, 5, 38, 125, 38, 172, 167, 38, 213, 168, 13, 65, 47, 13, 108, 5, 109, 38, 39, 5, 38, 313, 280, 38, 85, 38, 124, 134, 286, 38, 85, 38, 124, 38, 108, 309, 38, 136, 77, 211, 38, 125, 38, 307, 38, 308, 25, 312, 4, 44, 167, 192, 168, 25, 35, 25, 30, 4, 12, 4, 2642, 622, 3\n",
      "\n",
      "2021-01-28 20:00:12,447 : MainThread : INFO : Tokenizing training data\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 46271.74it/s]\n",
      "2021-01-28 20:00:12,881 : MainThread : INFO : Creating and shuffling tensorflow dataset\n",
      "2021-01-28 20:00:14,295 : MainThread : INFO : Initializing synthetic model\n",
      "2021-01-28 20:00:14,684 : MainThread : INFO : Using keras.optimizers.RMSprop optimizer\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (64, None, 256)           2764800   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (64, None, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (64, None, 256)           525312    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (64, None, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (64, None, 256)           525312    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (64, None, 256)           0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (64, None, 10800)         2775600   \n",
      "=================================================================\n",
      "Total params: 6,591,024\n",
      "Trainable params: 6,591,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "98/98 [==============================] - 11s 82ms/step - loss: 4.4258 - accuracy: 0.2667\n",
      "Epoch 2/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 1.4871 - accuracy: 0.6833\n",
      "Epoch 3/100\n",
      "98/98 [==============================] - 9s 83ms/step - loss: 0.7191 - accuracy: 0.8641\n",
      "Epoch 4/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.6579 - accuracy: 0.8731\n",
      "Epoch 5/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.6227 - accuracy: 0.8788\n",
      "Epoch 6/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.6072 - accuracy: 0.8817\n",
      "Epoch 7/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.5982 - accuracy: 0.8827\n",
      "Epoch 8/100\n",
      "98/98 [==============================] - 9s 81ms/step - loss: 0.5807 - accuracy: 0.8854\n",
      "Epoch 9/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.5740 - accuracy: 0.8865\n",
      "Epoch 10/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.5629 - accuracy: 0.8891\n",
      "Epoch 11/100\n",
      "98/98 [==============================] - 9s 83ms/step - loss: 0.5530 - accuracy: 0.8908\n",
      "Epoch 12/100\n",
      "98/98 [==============================] - 9s 83ms/step - loss: 0.5464 - accuracy: 0.8941\n",
      "Epoch 13/100\n",
      "98/98 [==============================] - 9s 83ms/step - loss: 0.5391 - accuracy: 0.8949\n",
      "Epoch 14/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.5341 - accuracy: 0.8961\n",
      "Epoch 15/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.5259 - accuracy: 0.8965\n",
      "Epoch 16/100\n",
      "98/98 [==============================] - 9s 83ms/step - loss: 0.5189 - accuracy: 0.8977\n",
      "Epoch 17/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.5139 - accuracy: 0.8984\n",
      "Epoch 18/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.5085 - accuracy: 0.8980\n",
      "Epoch 19/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.5063 - accuracy: 0.8983\n",
      "Epoch 20/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4992 - accuracy: 0.8991\n",
      "Epoch 21/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4967 - accuracy: 0.8997\n",
      "Epoch 22/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4958 - accuracy: 0.8996\n",
      "Epoch 23/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4896 - accuracy: 0.9006\n",
      "Epoch 24/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4866 - accuracy: 0.9006\n",
      "Epoch 25/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4831 - accuracy: 0.9012\n",
      "Epoch 26/100\n",
      "98/98 [==============================] - 9s 81ms/step - loss: 0.4801 - accuracy: 0.9019\n",
      "Epoch 27/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4753 - accuracy: 0.9020\n",
      "Epoch 28/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4739 - accuracy: 0.9020\n",
      "Epoch 29/100\n",
      "98/98 [==============================] - 9s 83ms/step - loss: 0.4729 - accuracy: 0.9026\n",
      "Epoch 30/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4670 - accuracy: 0.9039\n",
      "Epoch 31/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4679 - accuracy: 0.9037\n",
      "Epoch 32/100\n",
      "98/98 [==============================] - 9s 81ms/step - loss: 0.4658 - accuracy: 0.9038\n",
      "Epoch 33/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4608 - accuracy: 0.9045\n",
      "Epoch 34/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4637 - accuracy: 0.9047\n",
      "Epoch 35/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4608 - accuracy: 0.9056\n",
      "Epoch 36/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4590 - accuracy: 0.9051\n",
      "Epoch 37/100\n",
      "98/98 [==============================] - 9s 81ms/step - loss: 0.4574 - accuracy: 0.9060\n",
      "Epoch 38/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4558 - accuracy: 0.9058\n",
      "Epoch 39/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4557 - accuracy: 0.9064\n",
      "Epoch 40/100\n",
      "98/98 [==============================] - 9s 81ms/step - loss: 0.4526 - accuracy: 0.9068\n",
      "Epoch 41/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4544 - accuracy: 0.9067\n",
      "Epoch 42/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4514 - accuracy: 0.9074\n",
      "Epoch 43/100\n",
      "98/98 [==============================] - 9s 81ms/step - loss: 0.4484 - accuracy: 0.9082\n",
      "Epoch 44/100\n",
      "98/98 [==============================] - 9s 83ms/step - loss: 0.4512 - accuracy: 0.9080\n",
      "Epoch 45/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4478 - accuracy: 0.9080\n",
      "Epoch 46/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4453 - accuracy: 0.9089\n",
      "Epoch 47/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4496 - accuracy: 0.9083\n",
      "Epoch 48/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4477 - accuracy: 0.9086\n",
      "Epoch 49/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4445 - accuracy: 0.9095\n",
      "Epoch 50/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4457 - accuracy: 0.9096\n",
      "Epoch 51/100\n",
      "98/98 [==============================] - 9s 83ms/step - loss: 0.4477 - accuracy: 0.9094\n",
      "Epoch 52/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4467 - accuracy: 0.9099\n",
      "Epoch 53/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4491 - accuracy: 0.9100\n",
      "Epoch 54/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4469 - accuracy: 0.9106\n",
      "Epoch 55/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4442 - accuracy: 0.9107\n",
      "Epoch 56/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4446 - accuracy: 0.9110\n",
      "Epoch 57/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4420 - accuracy: 0.9109\n",
      "Epoch 58/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4447 - accuracy: 0.9105\n",
      "Epoch 59/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4436 - accuracy: 0.9117\n",
      "Epoch 60/100\n",
      "98/98 [==============================] - 9s 82ms/step - loss: 0.4448 - accuracy: 0.9107\n",
      "2021-01-28 20:09:14,880 : MainThread : INFO : Saving model history to model_history.csv\n",
      "2021-01-28 20:09:14,883 : MainThread : INFO : Saving model to /home/temesghen/projects/gretel-synthetics/examples/checkpoints/batch_2/synthetic\n"
     ]
    }
   ],
   "source": [
    "# Now we can trigger each batch to train\n",
    "batcher.train_all_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we can trigger all batched models to create output. This loops over each model and will attempt to generate\n",
    "# ``gen_lines`` valid lines for each model. This method returns a dictionary of bools that is indexed by batch number\n",
    "# and tells us if, for each batch, we were able to generate the requested number of valid lines\n",
    "status = batcher.generate_all_batch_lines(num_lines=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batcher.batches[2].gen_data_stream.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can grab a DataFrame for each batch index\n",
    "batcher.batch_to_df(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we can re-assemble all synthetic batches into our new synthetic DF\n",
    "batcher.batches_to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read only mode\n",
    "\n",
    "If you've already created a model(s) and simply want to load that data to generate more lines, you can use the read-only mode for the batch interface. No input DataFrame is required and it will automatically try and load model information from a primary checkpoint directory.\n",
    "\n",
    "Additionally, you can also control the number of lines you wish to generate with the ``num_lines`` parameter for generation. This option exists for write mode as well and overrides the number of lines specified in the synthetic config that was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_batch = DataFrameBatch(mode=\"read\", checkpoint_dir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_batch.generate_all_batch_lines(num_lines=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_batch.batches_to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('env3.7.9-synt')",
   "metadata": {
    "interpreter": {
     "hash": "a868afb3c14d86b874414c30d76d93c7c03424bbee27a51e62d46851d8669099"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}